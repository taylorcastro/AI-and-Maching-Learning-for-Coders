{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.0.ai_and_maching_learning_for_coders.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBUbU6PhRlq826MNy4yn8G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taylorcastro/ai_and_maching_learning_for_coders/blob/main/2.0.ai_and_maching_learning_for_coders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "055FHECp0ixl"
      },
      "source": [
        "#Chapter 2. Introduction to Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_BywiXO0F8U"
      },
      "source": [
        "##Recognizing Clothing Items\n",
        "##The Data: Fashion MNIST\n",
        "##Neurons for Vision\n",
        "##Designing the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBVunfU4tlX2",
        "outputId": "5c213827-fca1-4089-b911-acda96d8e97e"
      },
      "source": [
        "import tensorflow as tf\n",
        "data = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
        "\n",
        "training_images  = training_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "            tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "            tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "            #tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "            #tf.keras.layers.Dropout(0.2),\n",
        "            #tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "            #tf.keras.layers.Dropout(0.2),\n",
        "        ])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6492 - accuracy: 0.7728\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3853 - accuracy: 0.8617\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3424 - accuracy: 0.8768\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3134 - accuracy: 0.8858\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2922 - accuracy: 0.8921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0c83dcb4d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZJoWJdM9sk1",
        "outputId": "1e326760-37d5-41df-83b4-3f5b7ef536db"
      },
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 1ms/step - loss: 0.3702 - accuracy: 0.8669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37018144130706787, 0.8669000267982483]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyNIWu5102BT"
      },
      "source": [
        "Exploring the Model Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1zb6_57xd9U",
        "outputId": "1397a1fa-bb2f-42cd-cc27-d634f3b73404"
      },
      "source": [
        "classifications = model.predict(test_images)\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.7189217e-06 1.6440415e-08 5.4224387e-08 3.3804894e-09 1.1076262e-06\n",
            " 5.7157595e-03 5.0068905e-07 5.2702527e-02 2.1110134e-06 9.4157624e-01]\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgtxUVOq523u",
        "outputId": "7d917084-5eab-4eb8-f38a-6192af6f69f1"
      },
      "source": [
        "model.fit(training_images, training_labels, epochs=50)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2817 - accuracy: 0.8952\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2698 - accuracy: 0.9001\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2577 - accuracy: 0.9041\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2474 - accuracy: 0.9092\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2375 - accuracy: 0.9113\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2324 - accuracy: 0.9128\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2231 - accuracy: 0.9171\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2180 - accuracy: 0.9189\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2104 - accuracy: 0.9200\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2045 - accuracy: 0.9240\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1971 - accuracy: 0.9259\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1928 - accuracy: 0.9280\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1859 - accuracy: 0.9307\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1839 - accuracy: 0.9309\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1797 - accuracy: 0.9319\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1760 - accuracy: 0.9338\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1690 - accuracy: 0.9367\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1646 - accuracy: 0.9380\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1614 - accuracy: 0.9398\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1570 - accuracy: 0.9414\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1527 - accuracy: 0.9425\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1498 - accuracy: 0.9435\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1465 - accuracy: 0.9448\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1439 - accuracy: 0.9461\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1400 - accuracy: 0.9480\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1386 - accuracy: 0.9480\n",
            "Epoch 27/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1348 - accuracy: 0.9499\n",
            "Epoch 28/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1310 - accuracy: 0.9517\n",
            "Epoch 29/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1306 - accuracy: 0.9511\n",
            "Epoch 30/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1280 - accuracy: 0.9525\n",
            "Epoch 31/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1240 - accuracy: 0.9536\n",
            "Epoch 32/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1237 - accuracy: 0.9538\n",
            "Epoch 33/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1188 - accuracy: 0.9555\n",
            "Epoch 34/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1184 - accuracy: 0.9554\n",
            "Epoch 35/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1150 - accuracy: 0.9571\n",
            "Epoch 36/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1144 - accuracy: 0.9579\n",
            "Epoch 37/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1127 - accuracy: 0.9580\n",
            "Epoch 38/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1115 - accuracy: 0.9578\n",
            "Epoch 39/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1071 - accuracy: 0.9609\n",
            "Epoch 40/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1067 - accuracy: 0.9604\n",
            "Epoch 41/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1039 - accuracy: 0.9612\n",
            "Epoch 42/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1043 - accuracy: 0.9611\n",
            "Epoch 43/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1030 - accuracy: 0.9618\n",
            "Epoch 44/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1011 - accuracy: 0.9624\n",
            "Epoch 45/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0977 - accuracy: 0.9638\n",
            "Epoch 46/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0962 - accuracy: 0.9642\n",
            "Epoch 47/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0951 - accuracy: 0.9640\n",
            "Epoch 48/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0917 - accuracy: 0.9657\n",
            "Epoch 49/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0946 - accuracy: 0.9647\n",
            "Epoch 50/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0869 - accuracy: 0.9682\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0c80e4c190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFBm-vXd5h62"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUWXMfFq1P_Y"
      },
      "source": [
        "##Stopping Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbm4-qRq6vKW"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.95):\n",
        "      print(\"\\nReached 95% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "mnist = tf.keras.datasets.fashion_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjuNY-pV3abV"
      },
      "source": [
        "##Fashion CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLnR3-Du0s_a",
        "outputId": "8c578973-60f1-4ef3-cb73-f717bfe08199"
      },
      "source": [
        "import tensorflow as tf\n",
        "data = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.99):\n",
        "      print(\"\\nReached 99% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
        "print(type(training_images))\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images  = training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=50, callbacks=[callbacks])\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "<class 'numpy.ndarray'>\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 86s 45ms/step - loss: 0.6037 - accuracy: 0.7824\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 84s 45ms/step - loss: 0.3059 - accuracy: 0.8879\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 84s 45ms/step - loss: 0.2508 - accuracy: 0.9066\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 84s 45ms/step - loss: 0.2236 - accuracy: 0.9161\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.1905 - accuracy: 0.9293\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.1635 - accuracy: 0.9393\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 84s 45ms/step - loss: 0.1452 - accuracy: 0.9458\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.1244 - accuracy: 0.9537\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.1095 - accuracy: 0.9585\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.0948 - accuracy: 0.9633\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.0842 - accuracy: 0.9686\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.0741 - accuracy: 0.9715\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.0636 - accuracy: 0.9761\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.0571 - accuracy: 0.9783\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 82s 44ms/step - loss: 0.0524 - accuracy: 0.9806\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.0465 - accuracy: 0.9835\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.0448 - accuracy: 0.9840\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.0372 - accuracy: 0.9865\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 83s 44ms/step - loss: 0.0379 - accuracy: 0.9852\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 83s 45ms/step - loss: 0.0356 - accuracy: 0.9869\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 84s 45ms/step - loss: 0.0308 - accuracy: 0.9890\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 87s 46ms/step - loss: 0.0274 - accuracy: 0.9897\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 88s 47ms/step - loss: 0.0266 - accuracy: 0.9906\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 88s 47ms/step - loss: 0.0269 - accuracy: 0.9905\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 89s 47ms/step - loss: 0.0305 - accuracy: 0.9889\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 89s 47ms/step - loss: 0.0242 - accuracy: 0.9913\n",
            "\n",
            "Reached 99% accuracy so cancelling training!\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 0.6076 - accuracy: 0.9077\n",
            "[3.5425819e-20 4.4827975e-24 1.6607701e-17 3.4947880e-18 1.9047501e-25\n",
            " 8.0887154e-14 3.9479560e-18 3.7488384e-14 8.6040258e-21 1.0000000e+00]\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWJXrqIA38Ed"
      },
      "source": [
        "#Fashion mnist withcallbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrWCcj4K3tM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14962028-2e10-48d5-fd45-5b819c5fb03f"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.95):\n",
        "      print(\"\\nReached 95% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images/255.0\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, callbacks=[callbacks])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6261 - accuracy: 0.7850\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3857 - accuracy: 0.8603\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3361 - accuracy: 0.8766\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3155 - accuracy: 0.8854\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2946 - accuracy: 0.8924\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2768 - accuracy: 0.8987\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2615 - accuracy: 0.9038\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2545 - accuracy: 0.9052\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2447 - accuracy: 0.9088\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2358 - accuracy: 0.9111\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2269 - accuracy: 0.9142\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2217 - accuracy: 0.9172\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2169 - accuracy: 0.9192\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2093 - accuracy: 0.9211\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2052 - accuracy: 0.9234\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2023 - accuracy: 0.9249\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1906 - accuracy: 0.9282\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1893 - accuracy: 0.9276\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1857 - accuracy: 0.9303\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1761 - accuracy: 0.9347\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1715 - accuracy: 0.9370\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1604 - accuracy: 0.9380\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1631 - accuracy: 0.9387\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1568 - accuracy: 0.9418\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1597 - accuracy: 0.9402\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1470 - accuracy: 0.9432\n",
            "Epoch 27/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1475 - accuracy: 0.9451\n",
            "Epoch 28/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1443 - accuracy: 0.9446\n",
            "Epoch 29/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1386 - accuracy: 0.9471\n",
            "Epoch 30/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1427 - accuracy: 0.9472\n",
            "Epoch 31/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1365 - accuracy: 0.9499\n",
            "Epoch 32/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1348 - accuracy: 0.9492\n",
            "Epoch 33/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1311 - accuracy: 0.9503\n",
            "Epoch 34/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1267 - accuracy: 0.9522\n",
            "\n",
            "Reached 95% accuracy so cancelling training!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f12979f8cd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bw2AqSb72K3"
      },
      "source": [
        "## H or H CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCDat2vn77uN",
        "outputId": "107bf629-77d4-4394-91a8-d02b56ece5c0"
      },
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "training_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\n",
        "training_file_name = \"horse-or-human.zip\"\n",
        "training_dir = 'horse-or-human/training/'\n",
        "urllib.request.urlretrieve(training_url, training_file_name)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(training_file_name, 'r')\n",
        "zip_ref.extractall(training_dir)\n",
        "zip_ref.close()\n",
        "\n",
        "validation_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\"\n",
        "validation_file_name = \"validation-horse-or-human.zip\"\n",
        "validation_dir = 'horse-or-human/validation/'\n",
        "urllib.request.urlretrieve(validation_url, validation_file_name)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(validation_file_name, 'r')\n",
        "zip_ref.extractall(validation_dir)\n",
        "zip_ref.close()\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        training_dir,\n",
        "        target_size=(300, 300),\n",
        "        class_mode='binary')\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(300, 300),\n",
        "        class_mode='binary')\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=15,\n",
        "    validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 298, 298, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 149, 149, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 147, 147, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 71, 71, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 33, 33, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,704,097\n",
            "Trainable params: 1,704,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "33/33 [==============================] - 95s 3s/step - loss: 0.6856 - accuracy: 0.5849 - val_loss: 1.4067 - val_accuracy: 0.7812\n",
            "Epoch 2/15\n",
            "33/33 [==============================] - 94s 3s/step - loss: 0.3019 - accuracy: 0.8989 - val_loss: 2.9630 - val_accuracy: 0.7344\n",
            "Epoch 3/15\n",
            "33/33 [==============================] - 94s 3s/step - loss: 0.2204 - accuracy: 0.9227 - val_loss: 0.4219 - val_accuracy: 0.9023\n",
            "Epoch 4/15\n",
            "33/33 [==============================] - 94s 3s/step - loss: 0.1234 - accuracy: 0.9609 - val_loss: 0.9516 - val_accuracy: 0.8164\n",
            "Epoch 5/15\n",
            "33/33 [==============================] - 94s 3s/step - loss: 0.0240 - accuracy: 0.9944 - val_loss: 0.6124 - val_accuracy: 0.9023\n",
            "Epoch 6/15\n",
            "33/33 [==============================] - 93s 3s/step - loss: 0.6526 - accuracy: 0.9207 - val_loss: 2.3028 - val_accuracy: 0.8125\n",
            "Epoch 7/15\n",
            "33/33 [==============================] - 94s 3s/step - loss: 0.0280 - accuracy: 0.9908 - val_loss: 3.8717 - val_accuracy: 0.7188\n",
            "Epoch 8/15\n",
            "33/33 [==============================] - 94s 3s/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.5690 - val_accuracy: 0.8281\n",
            "Epoch 9/15\n",
            "33/33 [==============================] - 94s 3s/step - loss: 0.1719 - accuracy: 0.9873 - val_loss: 3.8878 - val_accuracy: 0.8203\n",
            "Epoch 10/15\n",
            "33/33 [==============================] - 94s 3s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.9527 - val_accuracy: 0.8594\n",
            "Epoch 11/15\n",
            "33/33 [==============================] - 94s 3s/step - loss: 1.1034e-04 - accuracy: 1.0000 - val_loss: 2.9875 - val_accuracy: 0.8633\n",
            "Epoch 12/15\n",
            "33/33 [==============================] - 94s 3s/step - loss: 0.7498 - accuracy: 0.9866 - val_loss: 2.9956 - val_accuracy: 0.8125\n",
            "Epoch 13/15\n",
            "33/33 [==============================] - 94s 3s/step - loss: 0.0041 - accuracy: 0.9960 - val_loss: 2.6529 - val_accuracy: 0.8398\n",
            "Epoch 14/15\n",
            "33/33 [==============================] - 98s 3s/step - loss: 1.6912e-04 - accuracy: 1.0000 - val_loss: 2.8714 - val_accuracy: 0.8398\n",
            "Epoch 15/15\n",
            "33/33 [==============================] - 93s 3s/step - loss: 1.7437e-05 - accuracy: 1.0000 - val_loss: 3.1140 - val_accuracy: 0.8633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgyLtwax9N_z"
      },
      "source": [
        "#Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JjQwdwX9UGY",
        "outputId": "7d1debd6-0104-4492-b275-b6d327902e1c"
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "data_url = \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\"\n",
        "data_file_name = \"catsdogs.zip\"\n",
        "download_dir = 'tmp/'\n",
        "urllib.request.urlretrieve(data_url, data_file_name)\n",
        "zip_ref = zipfile.ZipFile(data_file_name, 'r')\n",
        "zip_ref.extractall(download_dir)\n",
        "zip_ref.close()\n",
        "\n",
        "print(len(os.listdir('tmp/PetImages/Cat/')))\n",
        "print(len(os.listdir('tmp/PetImages/Dog/')))\n",
        "\n",
        "\n",
        "try:\n",
        "    os.mkdir('tmp/cats-v-dogs')\n",
        "    os.mkdir('tmp/cats-v-dogs/training')\n",
        "    os.mkdir('tmp/cats-v-dogs/testing')\n",
        "    os.mkdir('tmp/cats-v-dogs/training/cats')\n",
        "    os.mkdir('tmp/cats-v-dogs/training/dogs')\n",
        "    os.mkdir('tmp/cats-v-dogs/testing/cats')\n",
        "    os.mkdir('tmp/cats-v-dogs/testing/dogs')\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "\n",
        "import random\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    files = []\n",
        "    for filename in os.listdir(SOURCE):\n",
        "        file = SOURCE + filename\n",
        "        if os.path.getsize(file) > 0:\n",
        "            files.append(filename)\n",
        "        else:\n",
        "            print(filename + \" is zero length, so ignoring.\")\n",
        "\n",
        "    training_length = int(len(files) * SPLIT_SIZE)\n",
        "    testing_length = int(len(files) - training_length)\n",
        "    shuffled_set = random.sample(files, len(files))\n",
        "    training_set = shuffled_set[0:training_length]\n",
        "    testing_set = shuffled_set[:testing_length]\n",
        "\n",
        "    for filename in training_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = TRAINING + filename\n",
        "        copyfile(this_file, destination)\n",
        "\n",
        "    for filename in testing_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = TESTING + filename\n",
        "        copyfile(this_file, destination)\n",
        "\n",
        "\n",
        "CAT_SOURCE_DIR = \"tmp/PetImages/Cat/\"\n",
        "TRAINING_CATS_DIR = \"tmp/cats-v-dogs/training/cats/\"\n",
        "TESTING_CATS_DIR = \"tmp/cats-v-dogs/testing/cats/\"\n",
        "DOG_SOURCE_DIR = \"tmp/PetImages/Dog/\"\n",
        "TRAINING_DOGS_DIR = \"tmp/cats-v-dogs/training/dogs/\"\n",
        "TESTING_DOGS_DIR = \"tmp/cats-v-dogs/testing/dogs/\"\n",
        "\n",
        "split_size = .9\n",
        "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
        "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n",
        "\n",
        "# Expected output\n",
        "# 666.jpg is zero length, so ignoring\n",
        "# 11702.jpg is zero length, so ignoring\n",
        "\n",
        "print(len(os.listdir('tmp/cats-v-dogs/training/cats/')))\n",
        "print(len(os.listdir('tmp/cats-v-dogs/training/dogs/')))\n",
        "print(len(os.listdir('tmp/cats-v-dogs/testing/cats/')))\n",
        "print(len(os.listdir('tmp/cats-v-dogs/testing/dogs/')))\n",
        "\n",
        "\n",
        "TRAINING_DIR = \"tmp/cats-v-dogs/training/\"\n",
        "# Experiment with your own parameters here to really try to drive it to 99.9% accuracy or better\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=100,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))\n",
        "\n",
        "VALIDATION_DIR = \"tmp/cats-v-dogs/testing/\"\n",
        "# Experiment with your own parameters here to really try to drive it to 99.9% accuracy or better\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=100,\n",
        "                                                              class_mode='binary',\n",
        "                                                              target_size=(150, 150))\n",
        "\n",
        "\n",
        "weights_url = \"https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "weights_file = \"inception_v3.h5\"\n",
        "urllib.request.urlretrieve(weights_url, weights_file)\n",
        "\n",
        "pre_trained_model = InceptionV3(input_shape=(150, 150, 3),\n",
        "                                include_top=False,\n",
        "                                weights=None)\n",
        "\n",
        "pre_trained_model.load_weights(weights_file)\n",
        "\n",
        "for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# pre_trained_model.summary()\n",
        "\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.3)(x)\n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(pre_trained_model.input, x)\n",
        "\n",
        "model.compile(optimizer=RMSprop(lr=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "            train_generator,\n",
        "            validation_data=validation_generator,\n",
        "            epochs=20,\n",
        "            verbose=1)\n",
        "\n",
        "model.save(\"catsdogs.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12501\n",
            "12501\n",
            "666.jpg is zero length, so ignoring.\n",
            "11702.jpg is zero length, so ignoring.\n",
            "11250\n",
            "11250\n",
            "1250\n",
            "1250\n",
            "Found 22499 images belonging to 2 classes.\n",
            "Found 2500 images belonging to 2 classes.\n",
            "last layer output shape:  (None, 7, 7, 768)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "185/225 [=======================>......] - ETA: 2:59 - loss: 0.4030 - acc: 0.8627"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "225/225 [==============================] - 1106s 5s/step - loss: 0.3726 - acc: 0.8707 - val_loss: 0.0734 - val_acc: 0.9720\n",
            "Epoch 2/20\n",
            "225/225 [==============================] - 1114s 5s/step - loss: 0.1584 - acc: 0.9345 - val_loss: 0.0687 - val_acc: 0.9732\n",
            "Epoch 3/20\n",
            "225/225 [==============================] - 1116s 5s/step - loss: 0.1424 - acc: 0.9438 - val_loss: 0.0861 - val_acc: 0.9680\n",
            "Epoch 4/20\n",
            "225/225 [==============================] - 1115s 5s/step - loss: 0.1445 - acc: 0.9425 - val_loss: 0.0613 - val_acc: 0.9764\n",
            "Epoch 5/20\n",
            "225/225 [==============================] - 1107s 5s/step - loss: 0.1282 - acc: 0.9482 - val_loss: 0.0810 - val_acc: 0.9720\n",
            "Epoch 6/20\n",
            "225/225 [==============================] - 1098s 5s/step - loss: 0.1330 - acc: 0.9486 - val_loss: 0.0589 - val_acc: 0.9776\n",
            "Epoch 7/20\n",
            "225/225 [==============================] - 1107s 5s/step - loss: 0.1298 - acc: 0.9510 - val_loss: 0.0632 - val_acc: 0.9800\n",
            "Epoch 8/20\n",
            "225/225 [==============================] - 1098s 5s/step - loss: 0.1250 - acc: 0.9530 - val_loss: 0.0651 - val_acc: 0.9768\n",
            "Epoch 9/20\n",
            "225/225 [==============================] - 1094s 5s/step - loss: 0.1218 - acc: 0.9502 - val_loss: 0.0582 - val_acc: 0.9804\n",
            "Epoch 10/20\n",
            "225/225 [==============================] - 1098s 5s/step - loss: 0.1185 - acc: 0.9563 - val_loss: 0.0607 - val_acc: 0.9784\n",
            "Epoch 11/20\n",
            "225/225 [==============================] - 1096s 5s/step - loss: 0.1176 - acc: 0.9572 - val_loss: 0.0603 - val_acc: 0.9784\n",
            "Epoch 12/20\n",
            "225/225 [==============================] - 1093s 5s/step - loss: 0.1105 - acc: 0.9572 - val_loss: 0.0550 - val_acc: 0.9828\n",
            "Epoch 13/20\n",
            " 60/225 [=======>......................] - ETA: 12:16 - loss: 0.1126 - acc: 0.9612"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}